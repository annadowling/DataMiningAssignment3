{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Association Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read Excel files, you might need to install the `xlrd` package, using something like:\n",
    "\n",
    "    conda activate myEnvironment # where myEnvironment is the conda environment you use for this module\n",
    "    conda install xlrd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may find the following useful to obtain the data from the UCI data repository, and to read it into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "import requests, os\n",
    "#csvUrl = \"http://www.sci.csueastbay.edu/~esuess/classes/Statistics_6620/Presentations/ml13/groceries.csv\"\n",
    "#csvFile = 'data/groceries.csv'\n",
    "xlUrl = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx'\n",
    "xlFile = 'data/Online Retail.xlsx'\n",
    "dataFile = xlFile\n",
    "url = xlUrl\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "if not os.path.isfile(dataFile):\n",
    "  r = requests.get(url)\n",
    "  with open(dataFile, 'wb') as f:\n",
    "    f.write(r.content)\n",
    "if (dataFile == xlFile):\n",
    "  df = pd.read_excel(dataFile)\n",
    "else:\n",
    "  df = pd.read_csv(dataFile)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 1.1__: Select the transactions arising from the `Country` having _9042_ records in the dataframe and convert them to the OneHotEncoded form, where each column has (0,1) values representing the (absence,presence) of that product in a given basket, where each basket (row) is labeled by its `InvoiceNo`.\n",
    "\n",
    "Hints\n",
    "1. Use `groupby` and `size()` to determined the number of rows per `Country`\n",
    "2. Use `groupby` and `sum()` on the `Quantity` to encode as 0 and positive integer, and `reset_index()` so that the rows are labeled by `InvoiceNo`. Remember to set any positive numbers to 1 rather than a frequency count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 1.2__: Use mlxtend's `apriori` function to find the frequent itemsets where the minimum support threshold is set to 0.02. Hence derive the association rules where the minimum lift threshold is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 1.3__: Defining the _rule length_ to be the total number of products in the rule, plot the distribution of association rules by rule length and explain why the distribution looks like it does. Choosing the _longest_ rules, find the most attractive rule for use when recommending a (set of) products to a customer. Explain why reversing the rule might not be as effective.\n",
    "\n",
    "_Hints_\n",
    "1. The rule length is the sum of the lengths of the `antecedents` and `consequents` per rule.\n",
    "2. Association rules can be used for recommendation to customers who have already bought the antecedent products and might be interested in buying the consequent products. Note that you will need to use (by sorting) and justify suitable association measures to choose the most attractive rule for recommendation purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender Systems\n",
    "\n",
    "We use the well-known MovieLens dataset (in this case the small version). You may find the following useful to obtain the data from the GroupLens repository, and to read it into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests\n",
    "import numpy as np\n",
    "\n",
    "#mlSize = \"ml-1m\"\n",
    "#mlSize = \"ml-100k\"\n",
    "mlSize = \"ml-latest-small\"\n",
    "zipUrl = 'http://files.grouplens.org/datasets/movielens/'+mlSize+'.zip'\n",
    "zipFile = 'data/'+mlSize+'.zip'\n",
    "dataFile = zipFile\n",
    "url = zipUrl\n",
    "dataDir = 'data'\n",
    "if not os.path.exists(dataDir):\n",
    "    os.makedirs(dataDir)\n",
    "if not os.path.isfile(zipFile):\n",
    "  r = requests.get(zipUrl)\n",
    "  with open(zipFile, 'wb') as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "# Need to unzip the file to read its contents\n",
    "import zipfile\n",
    "with zipfile.ZipFile(zipFile,\"r\") as zip_ref:\n",
    "  zip_ref.extractall(dataDir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2.1__: Read the `users.dat`, `movies.dat` and `ratings.dat` data files into data frames.\n",
    "\n",
    "_Hints_\n",
    "\n",
    "1. You may find Pandas `read_csv` provides most of what you need, although you will need to override its default `sep` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2.2__: Generate the distribution of ratings (number of user-movie ratings, per rating value).\n",
    "\n",
    "_Hints_\n",
    "\n",
    "1. You may find that `value_counts()` helps to count the ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code can be used to filter the number of Movies. Choosing a large threshold (like 1750) ensures that only \"blockbuster\" movies with that number of aggregate ratings will be considered. This is convenient (much reduced runtimes!) when developing your solution, but a less stringent threshold should be used for the result you hand in (100 is suggested). You should also apply similar filters to the Users.\n",
    "\n",
    "To apply this filter to the ratings dataframe, you might find the `isin(filteredSet)` function useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minMovieRatings = 1750\n",
    "#minMovieRatings = 100\n",
    "filterMovies = ratingsDf['MovieID'].value_counts() > minMovieRatings\n",
    "filterMovies = filterMovies[filterMovies].index.tolist()\n",
    "len(filterMovies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2.3__: Using the filtered ratings dataframe, count the ratings per User and plot this data in a histogram. You should do the same with the Movies and comment on the similarities and differences between the two distributions.\n",
    "\n",
    "_Hint_\n",
    "\n",
    "1. You might find the `groupby()` and `count()` functions suitable for generating the data you need. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2.4__: Repeat Task 2.3 above, but deriving the average ratings rather than their counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2.5__: Load the (filtered) movies ratings data from the dataframe we have been exploring into the preferred 3-column format used by the `scikit-suprise` package. Now benchmark the performance (in terms of RMS error, time to fit, and time to generate predictions for test data) of the `SVD()`, `SlopeOne()`, `NMF()`, `KNNBasic()` recommendation algorithms. Discuss the strengths and weaknesses of each algorithm, based on its benchmarked results.\n",
    "\n",
    "_Hints_\n",
    "\n",
    "1. `scikit-surprise` provides `Reader` and `Dataset` functions to load one dataframe from another.\n",
    "2. `scikit-surprise` also provides a `cross_validate` function that can be used to estimate the test error in the test data, using the requested error metric.\n",
    "3. When collecting the benchmark data, you might find it convenient to loop over the algorithms and to add the results for each algorithm as a row to your benchmark array or dataframe."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
